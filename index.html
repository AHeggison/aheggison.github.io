<!DOCTYPE html>
<html lang="en">
<head>
<title>Automating Reliable UV SO<sub>2</sub> Camera Retrievals For Volcanology</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<style>
* {
  box-sizing: border-box;
}

.header {
  padding: 30px;
  text-align: center;
  background: #ce5c02;
  color: white;
}

.navbar {
  overflow:hidden;
  background-color: #514943;
}

.navbar a.right {
  float: right;
  display: block;
  color: white;
  text-align: center;
  padding: 14px;
  text-decoration:none;
}

.navbar a:hover {
background-color: #7c6859;
}

body {
  font-family: Verdana, sans-serif;
}

.column {
  float: left;
  width: 33.33%;
  padding: 5px;
}
  
.row::after {
  content: "";
  clear: both;
  display: table;
}

.column2 {
  float: left;
  width: 50%;
  padding: 5px;
}

figcaption {
  background-color: #514943;
  color: white;
  padding: 0px;
  text-align: left;
}
  
</style> 

<body>
<div class="header">
<h1 style="font-size:30px;">Automating Reliable UV SO_2 Camera Retrievals For Volcanology</h1>
</div>

<div class="navbar">
<a href="#Contact" class="right">Contact/About Me</a>
</div>

<div class="row">
  <div class="column">
  <figure>
    <img src="WebpageImages/PiCamAtReventador.jpg" alt="PiCam UV Camera" style="width:100%">
    <figcaption>PiCam UV camera at El Reventador, Ecuador (Dr Thomas Wilkes, University of Sheffield)</figcaption>
  </figure>
  </div>
  <div class="column">
    <figure>
    <img src="WebpageImages/OriginalUV.png" alt="Raw UV Camera Data" style="width:100%">
    <figcaption>UV camera 310nm image</figcaption>
    </figure>
  </div>
  <div class="column">
    <figure>
    <img src="WebpageImages/Absorbance.png" alt="Converted to SO2 Absorbance" style="width:100%">
    <figcaption>After conversion to SO<sub>2</sub> optical depth</figcaption>
    </figure>
  </div>
</div>
<p>Exploring use of computer vision techniques to enable automated measurement of volcanic sulphur dioxide flux, and provide informed estimates of the uncertainty.</p> 

<h2>Project Background:</h2>
<p>Since the early 2000s, UV camera technology for measurement of volcanic sulphur dioxide emission rates has developed, allowing for an increasing number permanent installations. 
The Volcanology group at The University of Sheffield have designed low-cost PiCam units, with a variety of installations across the globe. 
Collectively these cameras return vast amounts of data, which is very time consuming to process manually. 
Multiple groups have developed techniques for automating some or all processing steps for these large batches of data (CITATIONS HERE).
In this project we aim to develop reliable techniques for understanding data quality, making the most of computer vision to inform processing decisions and enable improved uncertainty quantification. 
Techniques are explored an tested on data from six volcanoes around the globe, with the intention of selecting methods which generalise well to new locations or camera installations. 
<p>
<h2>First step: Data Quality Asessment</h2>
<p>Ideally, all retrieved data would look like the example above: well lit with a a clear background sky, defined plume and nothing obscuring the recording.
However, this isn't always the case...
<p>
<div class="row">
  <div class="column">
  <figure>
    <img src="WebpageImages/Cloud.png" alt="Cloud covers the top of the volcano." style="width:100%">
    <figcaption>Cloud can cover the volcano flank, obscuring the plume.</figcaption>
  </figure>
  </div>
  <div class="column">
    <figure>
    <img src="WebpageImages/Rain.png" alt="UV volcano image with raindrops on the lens." style="width:100%">
    <figcaption>Rain, ash or snow regularly affect the data at some locations.</figcaption>
    </figure>
  </div>
  <div class="column">
    <figure>
    <img src="WebpageImages/Brid!.png" alt="A silhouette of a bird in front of a cloudy scene." style="width:100%">
    <figcaption>Sometimes wildlife even gets in the way!</figcaption>
    </figure>
  </div>
</div>

<h2>Current Progress:</h2> 
<p>We've been exploring the use of convolutional neural networks to identify two of the cases illustrated above: an obscured camera lens, and cloud interacting with or obstructing the view of the plume.
Networks pre-trained on the ImageNet dataset are fine-tuned using manually classified samples, to output a "lens-obscurance" index and "cloud obscurance" index.
Models are being evaluated for ability to generalise to data from unseen volcano locations, and prediction accuracy on new data from volcanoes seen in training.
</p>
<div class="row">
  <div class="column2">
    <figure>
    <img src="WebpageImages/OnLensSetFullSplit_Accuracy.png" alt="Graph showing increasing trend in prediction accuracy against training epochs." style="width:100%">
    <figcaption>Accuracy of a model-in-develpoment classifying on-lens obscurance, increasing as the model is trained.</figcaption>
    </figure>
  </div>
  <div class="column2">
    <figure>
    <img src="WebpageImages/FGCloudSetFullSplit_Accuracy.png" alt="Graph showing increasing trend in prediction accuracy against training epochs, less smooth than figure on the left." style="width:100%">
    <figcaption>Accuracy of a model-in-development for classifying cloud interacting with plume.</figcaption>
    </figure>
  </div>
</div>
The training process for baseline versions of these models is illustrated above. The prediction accuracy on the validation set increases with training epochs until the model has learned all the helpful information it can from the given data, at which point the accuracy levels off or decreases. We hope to improve these results futher through hyperparameter tuning, and expansion of the labelled dataset (described below). Currently predictions are generally strong on the most and least obscured data, but there is more error near the decision boundary. The results vary at different volcanoes too, as shown below. 
<div class="row">
  <div class="column2">
    <figure>
    <img src="WebpageImages/Kilauea ValidationSet Cloud Index Predictions.png" alt="" style="width:100%">
    <figcaption>The predictions of the cloud index on data from Kīlauea are really bad! Cloud often slightly mixes with the plume, and doesn't lie over the flank so this data looks significantly different to other locations. </figcaption>
    </figure>
  </div>
  <div class="column2">
    <figure>
    <img src="WebpageImages/Merapi ValidationSet Cloud Index Predictions.png" alt="" style="width:100%">
    <figcaption>However at other locations, in this case Merapi, the accuracy is very strong. In particular, all of the manually labelled "Very" and "In Calc" (affecting the area where absorbance is calculated) data is identified strongly by the index.</figcaption>
    </figure>
  </div>
</div>

<div class="row">
  <div class="column2">
    We found that the main limitation on the performance of the models in many cases was the labelled training dataset given. 
    However, manually labelling the terabytes of data available is not possible, and would make model training impractical as the dataset size increased.
    Therefore we are testing the use of a procedure to select key informative samples to assign manual labels to: 
    the features which the models use to describe given data in thier intermediate laters were calculated for a pool of potential new data to be labelled, 
    these features were clustered. Samples were selected from clusters which had low proportion of their data labelled already, high inconsistency in true labels,
    and with preference to samples where the model-prediction disagreed with the majority class in that cluster. Adding these new samples increased the training set
    size by just over 10%, and initial tests have found that training with this additional data significantly improves the predictions. 
  </div>
  <div class="column2">
    <figure>
    <img src="WebpageImages/Dataset Expansion.png" alt="" style="width:100%">
    <figcaption>A 2D representation of the feature space, showing the location of unlabelled samples from the dataset as a whole, the original labelled dataset, and the additional data selected for labelling.</figcaption></figure>
  </div>
</div>
  
<h2>Future Work:</h2>
Next planned steps include experimentation with automation of further processing steps (e.g. plume segmentation for background light estimation), and uncertainty quantification. 
This area of work has potential for use with other UV volcano camera systems, aswell as wider application e.g. to visible wavelength volcano camera data.

<h2 id="Contact">Contact/About</h2>  
<p>Hi! I’m Alyssa, a PhD student at the University of Sheffield where I’m supervised by members of the Volcanology and Computer Vision Research Groups: 
Dr Thomas Wilkes (Primary), Dr Tom Pering and Dr Jefersson dos Santos
I began working on this project in 2024, after studying maths in Edinburgh where I worked on algorithms for fair division of cakes! 
Applications of computer science and maths especially interest me and I hope to contribute a useful tool for volcano monitoring with this project.</p>
Please contact me at: asheggison1@sheffield.ac.uk
</body>
  
</html>
